{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2113174/anaconda3/envs/fenicsx-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import numpy as np\n",
    "from elliptic_files.train_elliptic import samples_param\n",
    "from elliptic_files.FEM_Solver import FEMSolver\n",
    "\n",
    "obs, nthetas = 6, 100\n",
    "thetas  = samples_param(nthetas,nparam=2)\n",
    "fem_solver = FEMSolver(np.zeros(2),vert=50)\n",
    "obs_points = np.linspace(0.2,0.8,obs).reshape(-1,1)\n",
    "training_data = np.zeros((nthetas,obs ))\n",
    "\n",
    "for i,theta in enumerate(thetas):\n",
    "    fem_solver.theta = theta\n",
    "    fem_solver.solve()\n",
    "    training_data[i,:] = fem_solver.eval_at_points(obs_points).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import solve_triangular\n",
    "from scipy.optimize import minimize\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "class KernelFunction:\n",
    "    def __init__(self,kernel_type=\"squared_exponential\"):\n",
    "\n",
    "        # Supported kernels\n",
    "        supported_covariances = {\n",
    "            \"squared_exponential\": self.squared_exponential_cov,\n",
    "            \"grad_squared_exponential\": self.grad_squared_exponential_cov\n",
    "        }\n",
    "\n",
    "        if kernel_type in supported_covariances:\n",
    "            self.covariance = supported_covariances[kernel_type]\n",
    "        else:\n",
    "            raise ValueError(f\"Kernel type '{kernel_type}' is not supported.\")\n",
    "        \n",
    "    def euclidean_distance_matrix(self,x, y):\n",
    "        x_sq = jnp.sum(x ** 2, axis=1, keepdims=True)\n",
    "        y_sq = jnp.sum(y ** 2, axis=1, keepdims=True).T\n",
    "        xy = jnp.dot(x, y.T)\n",
    "        dist_sq = x_sq - 2 * xy + y_sq\n",
    "        return jnp.sqrt(jnp.maximum(dist_sq, 0.0))\n",
    "\n",
    "    def squared_exponential_cov(self,r, sigma, l):\n",
    "        return (sigma**2) * jnp.exp(-0.5 * (r/l) ** 2)\n",
    "\n",
    "    def grad_squared_exponential_cov(self, r, sigma, l):\n",
    "        return ((sigma/l)**2) *r * jnp.exp(-0.5 * (r/l) ** 2)\n",
    "    \n",
    "    def compute_covariance(self,X,params,Y = None):\n",
    "        Y = X if Y is None else Y\n",
    "        d = self.euclidean_distance_matrix(X, Y)\n",
    "        return self.covariance(d,*params)\n",
    "\n",
    "class GaussianProcess:\n",
    "    def __init__(self,X_train,Y_train, prior_mean =0, kernel = \"squared_exponential\"):\n",
    "\n",
    "        self.X_train = jnp.array(X_train, dtype=jnp.float64)\n",
    "        self.Y_train = jnp.array(Y_train, dtype=jnp.float64).reshape(-1)\n",
    "        self.spatial_obs = Y_train.shape[-1]\n",
    "        self.param_obs = X_train.shape[0]\n",
    "        self.dim_total = self.spatial_obs*self.param_obs\n",
    "\n",
    "        self.prior_mean = prior_mean\n",
    "        self.kernel = KernelFunction(kernel_type=kernel)\n",
    "        self.opt_params = None  # Store optimized parameters\n",
    "\n",
    "    def observed_kernel(self,params):\n",
    "        # Step 1: Compute covariance matrix\n",
    "        cov_matrix_ob = self.kernel.compute_covariance(self.X_train,params)\n",
    "        cov_matrix = jnp.kron(cov_matrix_ob, jnp.eye(self.spatial_obs, dtype=jnp.float64)) +  1e-10 * jnp.eye(self.dim_total , dtype=jnp.float64)\n",
    "        # Step 2: Cholesky decomposition\n",
    "        L = jnp.linalg.cholesky(cov_matrix)\n",
    "        idt = jnp.eye(L.shape[0])\n",
    "        # Step 3: Solve the triangular system directly\n",
    "        z = solve_triangular(L, idt, lower=True)\n",
    "        z_t = solve_triangular(L, idt, lower=True, trans=1)\n",
    "        return L, jnp.dot(z_t, z)\n",
    "\n",
    "    def neg_log_likelihood(self, params):\n",
    "        params = jnp.array(params, dtype=jnp.float64)\n",
    "        \n",
    "        L, cov_inv = self.observed_kernel(params)\n",
    "\n",
    "        # Compute log determinant of K via Cholesky: logdet(K) = 2 * sum(log(diag(L)))\n",
    "        logdet_K = 2.0 * jnp.sum(jnp.log(jnp.diag(L)))\n",
    "\n",
    "        return 0.5 * (logdet_K + jnp.dot( self.Y_train.T,jnp.dot(cov_inv, self.Y_train)) + self.dim_total * jnp.log(2 * jnp.pi))\n",
    "\n",
    "    def nll_grad(self, params):\n",
    "        \"\"\"Compute the gradient of the negative log-likelihood\"\"\"\n",
    "        return jax.grad(self.neg_log_likelihood)(params)\n",
    "    \n",
    "    def optimize_nll(self,init_params = jnp.array([1.0, 1.0])):\n",
    "        res = minimize(self.neg_log_likelihood,init_params,\n",
    "                        method=\"L-BFGS-B\",jac=self.nll_grad,bounds=[(1e-5, None)] * len(init_params))\n",
    "        self.opt_params = res.x  # Store optimized params\n",
    "        _,self.obs_cov_inv = self.observed_kernel(self.opt_params)\n",
    "    \n",
    "\n",
    "    def predict_mean(self, x_test):\n",
    "        cov_train_test_ind = self.kernel.compute_covariance(x_test,self.opt_params,Y = self.X_train)\n",
    "        cov_train_test = jnp.kron(cov_train_test_ind, jnp.eye(self.spatial_obs))  \n",
    "        return self.prior_mean + cov_train_test @ (self.obs_cov_inv @ (self.Y_train - self.prior_mean))\n",
    "    \n",
    "    \n",
    "    def predict_var(self, x_test):\n",
    "        cov_test_train_ind = self.kernel.compute_covariance(x_test, self.opt_params, Y=self.X_train)\n",
    "        cov_test_train = np.kron(cov_test_train_ind, np.eye(self.spatial_obs))\n",
    "\n",
    "        #cov_train_test_ind = self.kernel.compute_covariance(self.X_train, self.opt_params, Y=x_test)\n",
    "\n",
    "        cov_test_ind = self.kernel.compute_covariance(x_test, self.opt_params)\n",
    "        cov_test = np.kron(cov_test_ind, np.eye(self.spatial_obs))\n",
    "\n",
    "        pred_var = cov_test - cov_test_train @ (self.obs_cov_inv @ cov_test_train.T)\n",
    "\n",
    "        return pred_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import qmc\n",
    "\n",
    "class LD_cube:\n",
    "    \n",
    "    def __init__(self, range_para, dimen = 1):\n",
    "        \n",
    "        self.dimen = dimen\n",
    "        self.range_para = self.para_range_setter(range_para)\n",
    "        self.points, self.V = self.para_to_points()\n",
    "        \n",
    "    def para_range_setter(self, range_para):\n",
    "        \n",
    "        if not isinstance(range_para[0],list) and self.dimen == 1:\n",
    "            range_para = [range_para]\n",
    "        elif not isinstance(range_para[0],list) and isinstance(self.dimen,int):\n",
    "            range_para = [range_para] * self.dimen        \n",
    "        \n",
    "        return range_para\n",
    "    \n",
    "    def para_to_points(self):\n",
    "                    \n",
    "        samples =  self.generator()\n",
    "                \n",
    "        V = 1\n",
    "        for j in range(self.dimen):\n",
    "            samples[:,j] = (self.range_para[j][1]-self.range_para[j][0])*samples[:,j] + self.range_para[j][0]\n",
    "            V = V * (self.range_para[j][1]-self.range_para[j][0])\n",
    "        \n",
    "        \n",
    "        return samples, V\n",
    "    \n",
    "class Halton_cube(LD_cube):\n",
    "\n",
    "    def __init__(self, range_para, n, dimen = 1):\n",
    "        \n",
    "        self.n = n\n",
    "        super().__init__(range_para, dimen)\n",
    "    \n",
    "    def generator(self):\n",
    "    \n",
    "        sampler = qmc.Halton(d=self.dimen, scramble=False)\n",
    "        \n",
    "        return sampler.random(n=self.n)\n",
    "    \n",
    "class data:\n",
    "    \"\"\"\n",
    "    Atrributes:\n",
    "    k_dm: dimension of unkonwn parameters\n",
    "    d_tr: number of training points\n",
    "    n_ob: number of observation points\n",
    "    \n",
    "    U: d_tr*k_dm matrix, unknown parameters\n",
    "    Y: d_tr*n_ob matrix, observational data\n",
    "    xx: spatial points corresponding to observations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, U, Y, k_dm, xx = None):\n",
    "        \n",
    "        self.k_dm = k_dm\n",
    "        self.d_tr = int(U.size/k_dm)\n",
    "        self.n_ob = int(Y.size/self.d_tr)\n",
    "        \n",
    "        self.U = U\n",
    "        self.Y = Y\n",
    "        \n",
    "        self.yy = Y.reshape(-1,1)\n",
    "        self.xx = xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "\n",
    "class kernel_function:\n",
    "    def __init__(self, dimen):\n",
    "        \n",
    "        self.dimen = dimen\n",
    "        self.K = self.cov\n",
    "    \n",
    "    def parameter_formalizer(self, para):\n",
    "        \n",
    "        para = np.atleast_2d(para).reshape(-1,1)\n",
    "        \n",
    "        return para\n",
    "    \n",
    "    def input_formalizer(self, x):\n",
    "        \n",
    "        return np.atleast_2d(x).reshape(-1,self.dimen)\n",
    "    \n",
    "    def weighted_dist_mat(self, l, x1, x2 = None):\n",
    "        \n",
    "        if x2 is None:\n",
    "            x2 = np.copy(x1)\n",
    "            \n",
    "        x1 = np.atleast_2d(x1).reshape(-1,self.dimen)\n",
    "        x2 = np.atleast_2d(x2).reshape(-1,self.dimen)\n",
    " \n",
    "        return distance_matrix(x1/l,x2/l)\n",
    " \n",
    "    def cov(self, x1, x2, n_ob):\n",
    "        \n",
    "        raise NotImplementedError(\n",
    "            'Function cov not implemented. Please initilize a specific kernel function.')\n",
    "        \n",
    "    def cov_3d(self, x1, x2, n_ob):\n",
    "        cov_mat = self.cov(x1,x2)\n",
    "        a,b = np.shape(cov_mat)\n",
    "        \n",
    "        return np.broadcast_to(cov_mat,(n_ob,a,b))\n",
    "    \n",
    "    def cov_kron(self, x1, x2, n_ob, Spatial_cov = None):\n",
    "        \n",
    "        if Spatial_cov is None:\n",
    "            Spatial_cov = np.eye(n_ob)\n",
    "            \n",
    "        cov_mat = self.cov(x1,x2)\n",
    "        \n",
    "        return np.kron(Spatial_cov,cov_mat)\n",
    "    \n",
    "class kernel_squared_exponential(kernel_function):\n",
    "    \n",
    "    def __init__(self, sigma_square, l, dimen = 1):\n",
    "        \n",
    "        self.sigma_square = sigma_square      \n",
    "        self.l = self.parameter_formalizer(l)  \n",
    "        self.dimen = dimen\n",
    "        \n",
    "        super().__init__(dimen)\n",
    "        \n",
    "    def cov(self, x1, x2 = None):\n",
    "        \n",
    "        d = self.weighted_dist_mat(self.l, x1, x2)\n",
    "        \n",
    "        return self.sigma_square * np.exp(-d**2/2)\n",
    "    \n",
    "    def cov_prime(self, x1, x2):\n",
    "        \n",
    "        return (-(x1.T-x2.T)/(self.l)**2) * self.cov(x1,x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (6, 10) (6,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load the data using NumPy\n",
    "u_Theta = np.loadtxt(\"2D_trainingset_N10.txt\")\n",
    "\n",
    "\n",
    "# Number of spatial oberservation points\n",
    "dy = 6\n",
    "X = np.linspace(0,1,dy)\n",
    "\n",
    "# Convert to jax.numpy\n",
    "#u_Theta = jnp.array(data_np)\n",
    "\n",
    "N = 10\n",
    "Theta_train = Halton_cube([-1,1], N+1, 2).points[1:,:]\n",
    "\n",
    "print(Theta_train.shape,u_Theta.shape,X.shape)\n",
    "tr_data = data(Theta_train, u_Theta.T, 2, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, nthetas = 6, 10\n",
    "thetas  = samples_param(nthetas,nparam=2)\n",
    "fem_solver = FEMSolver(np.zeros(2),vert=50)\n",
    "obs_points = np.linspace(0.2,0.8,obs).reshape(-1,1)\n",
    "training_data = np.zeros((nthetas,obs ))\n",
    "\n",
    "for i,theta in enumerate(thetas):\n",
    "    fem_solver.theta = theta\n",
    "    fem_solver.solve()\n",
    "    training_data[i,:] = fem_solver.eval_at_points(obs_points).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gp_regression:\n",
    "    def __init__(self, kernel, tr_data, normalizer = 10**(-10), prior_mean = 0):\n",
    "        \n",
    "        self.k_dm = tr_data.k_dm\n",
    "        self.d_tr = tr_data.d_tr\n",
    "        self.n_ob = tr_data.n_ob\n",
    "        \n",
    "        self.mean = prior_mean\n",
    "        self.k = kernel\n",
    "        self.tr_data = tr_data\n",
    "                \n",
    "        self.normalizer = normalizer\n",
    "                \n",
    "        self.K_inv_y, self.K_tr_tr = self.inverse_precomputer()\n",
    "\n",
    "       \n",
    "    def inverse_precomputer(self):\n",
    "        \n",
    "        K_tr_tr = self.k.cov(self.tr_data.U)   \n",
    "        K_tr_tr = np.kron(K_tr_tr, np.eye(self.n_ob)) + self.normalizer * np.eye(self.n_ob*self.d_tr)\n",
    "\n",
    "        K_inv_y = np.linalg.solve(K_tr_tr, self.tr_data.yy - self.mean)\n",
    "        \n",
    "        return K_inv_y, K_tr_tr\n",
    "    \n",
    "    \n",
    "    def predict_mean(self, test_u):\n",
    "        \n",
    "        K_test_tr = self.k.cov(test_u,self.tr_data.U)\n",
    "        K_test_tr = np.kron(K_test_tr, np.eye(self.n_ob))#self.extend(K_test_tr)\n",
    "            \n",
    "        pred_mean = self.mean + K_test_tr @ self.K_inv_y\n",
    "                \n",
    "        return pred_mean\n",
    "    \n",
    "    \n",
    "    def predict_var(self, test_u):\n",
    "        \n",
    "        K_test_tr = np.kron(self.k.cov(test_u, self.tr_data.U),np.eye(self.n_ob))\n",
    "        K_tr_test = np.kron(self.k.cov(self.tr_data.U, test_u),np.eye(self.n_ob)) \n",
    "        \n",
    "        pred_var = np.kron(self.k.cov(test_u), np.eye(self.n_ob))- K_test_tr @ np.linalg.solve(self.K_tr_tr, K_tr_test)\n",
    "        \n",
    "        return pred_var\n",
    "    \n",
    "    def predict_vars(self, test_u):\n",
    "\n",
    "        pred_vars = np.zeros((len(test_u),self.n_ob,self.n_ob))\n",
    "        \n",
    "        i = 0\n",
    "        for u in test_u:\n",
    "            pred_vars[i,:,:] = self.predict_var(u)\n",
    "            i = i + 1\n",
    "        \n",
    "        return pred_vars\n",
    "\n",
    "    def pred_mean_prime(self, test_u, X = None):\n",
    "        \n",
    "        cov_prime = self.k.cov_prime(test_u,self.tr_data.U)\n",
    "        mean_prime = np.kron(cov_prime, np.eye(self.n_ob)) @ self.K_inv_y\n",
    "        \n",
    "        return mean_prime\n",
    "    \n",
    "    def pred_var_prime(self, test_u):\n",
    "        \n",
    "        pred_var_prime = np.zeros((self.k_dm,self.n_ob,self.n_ob))\n",
    "        cov_prime = self.k.cov_prime(test_u,self.tr_data.U)\n",
    "        \n",
    "        K_tr_test = np.kron(self.k.cov(self.tr_data.U, test_u), np.eye(self.n_ob))\n",
    "        var_prime = np.kron(cov_prime, np.eye(self.n_ob)) @ np.linalg.solve(self.K_tr_tr, K_tr_test) \n",
    "        \n",
    "        for i in range(self.k_dm):\n",
    "            pred_var_prime[i,:,:] = var_prime[i::self.k_dm,:]\n",
    "\n",
    "        return -2 * pred_var_prime\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Spc_Gpr(Gp_regression):\n",
    "    \n",
    "    def __init__(self, kernel, tr_data, kernel_x, normalizer = 10**(-10), prior_mean = 0):\n",
    "        \n",
    "        self.kx = kernel_x\n",
    "        self.xx = tr_data.xx\n",
    "        \n",
    "        super().__init__(kernel, tr_data, normalizer, prior_mean)\n",
    "        \n",
    "        \n",
    "    def inverse_precomputer(self):\n",
    "        \n",
    "        K_tr_tr = self.k.cov(self.tr_data.U)         \n",
    "        K_tr_tr = np.kron(K_tr_tr, self.kx.cov(self.xx)) + self.normalizer * np.eye(self.n_ob*self.d_tr)\n",
    "        \n",
    "        K_inv_y = np.linalg.solve(K_tr_tr, self.tr_data.yy - self.mean)\n",
    "        \n",
    "        return K_inv_y, K_tr_tr\n",
    "    \n",
    "    \n",
    "    def predict_mean(self, test_u, X = None):\n",
    "        \n",
    "        if X is None: X = self.xx\n",
    "        \n",
    "        K_test_tr = self.k.cov(test_u,self.tr_data.U)\n",
    "        K_test_tr = np.kron(K_test_tr, self.kx.cov(X, self.xx) )#self.extend(K_test_tr)\n",
    "            \n",
    "        pred_mean = self.mean + K_test_tr @ self.K_inv_y\n",
    "                \n",
    "        return pred_mean.reshape(len(X), -1)\n",
    "    \n",
    "    \n",
    "    def predict_var(self, test_u, X = None):\n",
    "        \n",
    "        if X is None: X = self.xx\n",
    "        \n",
    "        K_test_tr = np.kron(self.k.cov(test_u, self.tr_data.U),self.kx.cov(X, self.xx))\n",
    "        K_tr_test = np.kron(self.k.cov(self.tr_data.U, test_u),self.kx.cov(self.xx, X)) \n",
    "        \n",
    "        pred_var = np.kron(self.k.cov(test_u), self.kx.cov(X))- K_test_tr @ np.linalg.solve(self.K_tr_tr, K_tr_test)\n",
    "        \n",
    "        return pred_var\n",
    "\n",
    "    def pred_mean_prime(self, test_u, X = None):\n",
    "        \n",
    "        if X is None: X = self.xx\n",
    "        \n",
    "        cov_prime = self.k.cov_prime(test_u,self.tr_data.U)\n",
    "        mean_prime = np.kron(cov_prime, self.kx.cov(X, self.xx)) @ self.K_inv_y\n",
    "        \n",
    "        \n",
    "        return mean_prime\n",
    "    \n",
    "    def pred_var_prime(self, test_u, X = None):\n",
    "        \n",
    "        if X is None: X = self.xx\n",
    "        \n",
    "        pred_var_prime = np.zeros((self.k_dm,self.n_ob,self.n_ob))\n",
    "        cov_prime = self.k.cov_prime(test_u,self.tr_data.U)\n",
    "        \n",
    "        K_tr_test = np.kron(self.k.cov(self.tr_data.U, test_u),self.kx.cov(self.xx, X))\n",
    "        var_prime = np.kron(cov_prime, self.kx.cov(X, self.xx)) @ np.linalg.solve(self.K_tr_tr, K_tr_test) \n",
    "        \n",
    "        for i in range(self.k_dm):\n",
    "            pred_var_prime[i,:,:] = var_prime[i::self.k_dm,:]\n",
    "\n",
    "        return -2 * pred_var_prime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = data(thetas, training_data, 2, obs_points.reshape(-1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-39.94413384])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = kernel_squared_exponential(1,1, dimen = 2)\n",
    "Gpr = Gp_regression(kernel,tr_data)\n",
    "\n",
    "def neg_log_marginal_likelihood(hyp):  # ML_GPR\n",
    "\n",
    "    kernel = kernel_squared_exponential(hyp[0], hyp[1], dimen = 2)\n",
    "    Gpr = Gp_regression(kernel,tr_data)\n",
    "    \n",
    "    K = Gpr.K_tr_tr\n",
    "\n",
    "    eig_v = np.linalg.eigvals(K)\n",
    "\n",
    "    L = np.linalg.cholesky(K)\n",
    "    y = tr_data.yy\n",
    "    nlml = sum(np.log(np.diag(L))) + 0.5*np.matmul(y.T,np.linalg.solve(K,y))[0] + 0.5*N*np.log(2*np.pi)\n",
    "\n",
    "    return nlml\n",
    "\n",
    "neg_log_marginal_likelihood(np.array([1.0, 1.0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized hyperparameter: [1.57410073 4.23240861]\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "hyp = np.array([1,1])\n",
    "bound = [[10**(-10), 100] for i in range(len(hyp))]\n",
    "hyp1 = minimize(neg_log_marginal_likelihood, hyp, method = 'L-BFGS-B', bounds = bound)\n",
    "print(\"Optimized hyperparameter:\",hyp1.x)\n",
    "\n",
    "kernel = kernel_squared_exponential(*hyp1.x, dimen = 2)\n",
    "Gpr = Gp_regression(kernel,tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42518855],\n",
       "       [0.66964511],\n",
       "       [0.92114373],\n",
       "       [1.18377489],\n",
       "       [1.45010828],\n",
       "       [1.69917492]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gpr.predict_mean(np.array([[0.098, 0.430]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.26246152e-07, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 8.26246152e-07, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 8.26246153e-07, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.26246153e-07,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        8.26246153e-07, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 8.26246153e-07]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gpr.predict_var(np.array([[0.098, 0.430]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6.00279282, dtype=float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp = GaussianProcess(thetas,training_data)\n",
    "\n",
    "gp.neg_log_likelihood(jnp.array([1.0, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcess(thetas,training_data)\n",
    "\n",
    "gp.optimize_nll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.33301301, 4.32873114])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.opt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.42521981, 0.66970029, 0.92122171, 1.18386563, 1.4501936 ,\n",
       "       1.69923526], dtype=float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.predict_mean(jnp.array([[0.098, 0.430]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[8.16826559e-07, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 8.09484865e-07, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 8.00558080e-07, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 8.18160566e-07,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        8.12329220e-07, 0.00000000e+00],\n",
       "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 7.98481281e-07]], dtype=float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp.predict_var(jnp.array([[0.098, 0.430]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fenicsx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
