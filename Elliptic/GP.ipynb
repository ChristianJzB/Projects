{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2113174/anaconda3/envs/fenicsx-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the parent directory to the sys.path\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "import numpy as np\n",
    "from elliptic_files.train_elliptic import samples_param\n",
    "from elliptic_files.FEM_Solver import FEMSolver\n",
    "\n",
    "obs, nthetas = 6, 100\n",
    "thetas  = samples_param(nthetas,nparam=2)\n",
    "fem_solver = FEMSolver(np.zeros(2),vert=50)\n",
    "obs_points = np.linspace(0.2,0.8,obs).reshape(-1,1)\n",
    "training_data = np.zeros((nthetas,obs ))\n",
    "\n",
    "for i,theta in enumerate(thetas):\n",
    "    fem_solver.theta = theta\n",
    "    fem_solver.solve()\n",
    "    training_data[i,:] = fem_solver.eval_at_points(obs_points).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax.scipy.linalg import solve_triangular\n",
    "from scipy.optimize import minimize\n",
    "import jax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "class KernelFunction:\n",
    "    def __init__(self,kernel_type=\"squared_exponential\"):\n",
    "\n",
    "        # Supported kernels\n",
    "        supported_covariances = {\n",
    "            \"squared_exponential\": self.squared_exponential_cov,\n",
    "            \"grad_squared_exponential\": self.grad_squared_exponential_cov\n",
    "        }\n",
    "\n",
    "        if kernel_type in supported_covariances:\n",
    "            self.covariance = supported_covariances[kernel_type]\n",
    "        else:\n",
    "            raise ValueError(f\"Kernel type '{kernel_type}' is not supported.\")\n",
    "        \n",
    "    def euclidean_distance_matrix(self,x, y):\n",
    "        x_sq = jnp.sum(x ** 2, axis=1, keepdims=True)\n",
    "        y_sq = jnp.sum(y ** 2, axis=1, keepdims=True).T\n",
    "        xy = jnp.dot(x, y.T)\n",
    "        dist_sq = x_sq - 2 * xy + y_sq\n",
    "        return jnp.sqrt(jnp.maximum(dist_sq, 0.0))\n",
    "\n",
    "    def squared_exponential_cov(self,r, sigma, l):\n",
    "        return (sigma**2) * jnp.exp(-0.5 * (r/l) ** 2)\n",
    "\n",
    "    def grad_squared_exponential_cov(self, r, sigma, l):\n",
    "        return ((sigma/l)**2) *r * jnp.exp(-0.5 * (r/l) ** 2)\n",
    "    \n",
    "    def compute_covariance(self,X,params,Y = None):\n",
    "        Y = X if Y is None else Y\n",
    "        d = self.euclidean_distance_matrix(X, Y)\n",
    "        return self.covariance(d,*params)\n",
    "\n",
    "class GaussianProcess:\n",
    "    def __init__(self,X_train,Y_train, x_spc = None,  prior_mean =0, kernel = \"squared_exponential\"):\n",
    "\n",
    "        self.X_train = jnp.array(X_train, dtype=jnp.float64)\n",
    "        self.Y_train = jnp.array(Y_train, dtype=jnp.float64).reshape(-1)\n",
    "        self.spatial_obs = Y_train.shape[-1]\n",
    "        self.param_obs = X_train.shape[0]\n",
    "        self.dim_total = self.spatial_obs*self.param_obs\n",
    "\n",
    "        self.prior_mean = prior_mean\n",
    "        self.kernel = KernelFunction(kernel_type=kernel)\n",
    "        self.opt_params = None  # Store optimized parameters\n",
    "        self.x_spc = x_spc\n",
    "\n",
    "    def observed_kernel(self,params, params_spc = None):\n",
    "        # Step 1: Compute covariance matrix\n",
    "        spatial_obs_cov = jnp.eye(self.spatial_obs, dtype=jnp.float64) if self.x_spc is None else self.kernel.compute_covariance(self.x_spc,[1,params_spc])\n",
    "        cov_matrix_ob = self.kernel.compute_covariance(self.X_train,params)\n",
    "        cov_matrix = jnp.kron(cov_matrix_ob, spatial_obs_cov) +  1e-10 * jnp.eye(self.dim_total , dtype=jnp.float64)\n",
    "        # Step 2: Cholesky decomposition\n",
    "        L = jnp.linalg.cholesky(cov_matrix)\n",
    "        idt = jnp.eye(L.shape[0])\n",
    "        # Step 3: Solve the triangular system directly\n",
    "        z = solve_triangular(L, idt, lower=True)\n",
    "        z_t = solve_triangular(L, idt, lower=True, trans=1)\n",
    "        return L, jnp.dot(z_t, z)\n",
    "\n",
    "    def neg_log_likelihood(self, full_params):\n",
    "        if self.x_spc is not None:\n",
    "            params_spc = full_params[-1]\n",
    "            params = full_params[:-1]\n",
    "        else:\n",
    "            params = full_params\n",
    "            params_spc = None\n",
    "        \n",
    "        L, cov_inv = self.observed_kernel(params,params_spc)\n",
    "\n",
    "        # Compute log determinant of K via Cholesky: logdet(K) = 2 * sum(log(diag(L)))\n",
    "        logdet_K = 2.0 * jnp.sum(jnp.log(jnp.diag(L)))\n",
    "\n",
    "        return 0.5 * (logdet_K + jnp.dot( self.Y_train.T,jnp.dot(cov_inv, self.Y_train)) + self.dim_total * jnp.log(2 * jnp.pi))\n",
    "\n",
    "    def nll_grad(self, params):\n",
    "        \"\"\"Compute the gradient of the negative log-likelihood\"\"\"\n",
    "        return jax.grad(self.neg_log_likelihood)(params)\n",
    "    \n",
    "    def optimize_nll(self,init_params):\n",
    "        res = minimize(self.neg_log_likelihood,init_params,\n",
    "                        method=\"L-BFGS-B\",jac=self.nll_grad,bounds=[(1e-5, None)] * len(init_params))\n",
    "        self.opt_params = res.x  # Store optimized params\n",
    "\n",
    "        if self.x_spc is not None:\n",
    "            self.opt_params = res.x[:-1]\n",
    "            self.opt_params_spc = res.x[-1]\n",
    "            _, self.obs_cov_inv = self.observed_kernel(self.opt_params, self.opt_params_spc)\n",
    "            self.spatial_obs_cov = self.kernel.compute_covariance(self.x_spc,[1,self.opt_params_spc])\n",
    "        else:\n",
    "            self.opt_params = res.x\n",
    "            _, self.obs_cov_inv = self.observed_kernel(self.opt_params, None)\n",
    "            self.spatial_obs_cov = jnp.eye(self.spatial_obs, dtype=jnp.float64)\n",
    "\n",
    "        self.inv_dif = self.obs_cov_inv @ (self.Y_train - self.prior_mean)\n",
    "            \n",
    "\n",
    "    def predict_mean(self, x_test):\n",
    "        cov_train_test_ind = self.kernel.compute_covariance(x_test,self.opt_params,Y = self.X_train)\n",
    "        cov_train_test = jnp.kron(cov_train_test_ind, self.spatial_obs_cov)  \n",
    "        return self.prior_mean + cov_train_test @ self.inv_dif\n",
    "    \n",
    "    \n",
    "    def predict_var(self, x_test):\n",
    "        cov_test_train_ind = self.kernel.compute_covariance(x_test, self.opt_params, Y=self.X_train)\n",
    "        cov_test_train = np.kron(cov_test_train_ind, self.spatial_obs_cov)\n",
    "\n",
    "        #cov_train_test_ind = self.kernel.compute_covariance(self.X_train, self.opt_params, Y=x_test)\n",
    "        cov_test_ind = self.kernel.compute_covariance(x_test, self.opt_params)\n",
    "        cov_test = np.kron(cov_test_ind, self.spatial_obs_cov)\n",
    "\n",
    "        pred_var = cov_test - cov_test_train @ (self.obs_cov_inv @ cov_test_train.T)\n",
    "\n",
    "        return pred_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elliptic_files.FEM_Solver import RootFinder\n",
    "\n",
    "class Elliptic_PIGP:\n",
    "    def __init__(self,data_training,lam = 1 /4, kl_terms = 2,kernel = \"squared_exponential\", l=1,sigma =1):\n",
    "        # Training data\n",
    "        self.parameters_data = data_training[\"parameters_data\"]\n",
    "        self.solutions_data = data_training[\"solutions_data\"]\n",
    "        self.x_bc = data_training[\"x_bc\"]\n",
    "        self.y_bc = data_training[\"y_bc\"]\n",
    "        self.source_func_x = data_training[\"source_func_x\"].reshape(-1,1)\n",
    "        self.source_func_f_x = data_training[\"source_func_f_x\"]\n",
    "\n",
    "        # Roots for KL\n",
    "        self.finder = RootFinder(lam, kl_terms)\n",
    "        self.roots = jnp.array(self.finder.find_roots())\n",
    "\n",
    "        # Kernels\n",
    "        self.kernel_parameter = KernelFunction(kernel_type=kernel)\n",
    "        self.kernel_spatial = KernelFunction(kernel_type=kernel)\n",
    "        self.l = l\n",
    "        self.sigma = sigma\n",
    "\n",
    "    @property\n",
    "    def A(self):\n",
    "        \"\"\"Compute the A coefficients.\"\"\"\n",
    "        return jnp.sqrt(1 / ((1/8)*(5 + (self.roots / 2)**2) + \n",
    "                            (jnp.sin(2*self.roots) / (4*self.roots)) * ((self.roots / 4)**2 - 1) - (jnp.cos(2*self.roots)/8)))\n",
    "    @property\n",
    "    def an(self):\n",
    "        \"\"\"Compute the an values.\"\"\"\n",
    "        return jnp.sqrt(8 / (self.roots**2 + 16))\n",
    "    \n",
    "    def exp_kl_eval(self,theta, x):\n",
    "        \"\"\"\n",
    "        Eval exp(kl)\n",
    "        \"\"\"\n",
    "        # basis\n",
    "        basis =self.A[:,None]*self.an[:,None]*(jnp.sin(self.roots[:, None] * x) + \\\n",
    "                                               (self.roots[:, None] / 4) * jnp.cos(self.roots[:, None] * x))\n",
    "        # Final result: shape (n, m) = (n, r) @ (r, m)\n",
    "        result = theta @ basis\n",
    "\n",
    "        return jnp.exp(result)\n",
    "    \n",
    "    def grad_kl_eval(self,theta, x):\n",
    "        \"\"\"\n",
    "        kl'\n",
    "        \"\"\"\n",
    "        # basis\n",
    "        basis =self.A[:,None]*self.an[:,None]*self.roots[:, None]*(jnp.cos(self.roots[:, None] * x) - \\\n",
    "                                               (self.roots[:, None] / 4) * jnp.sin(self.roots[:, None] * x))\n",
    "        # Final result: shape (n, m) = (n, r) @ (r, m)\n",
    "        result = theta @ basis\n",
    "\n",
    "        return result\n",
    "\n",
    "    def kernel_uf(self,theta,x):\n",
    "        x = x.reshape(1,-1)\n",
    "\n",
    "        cov = self.kernel_spatial.compute_covariance(x.reshape(-1,1),Y= self.source_func_x, params=[self.l,self.sigma])\n",
    "        exp_kl = self.exp_kl_eval(theta,x)\n",
    "        grad_kl = self.grad_kl_eval(theta,x)\n",
    "\n",
    "        delta_x = x - self.source_func_x\n",
    "        print(delta_x.shape,grad_kl.shape,exp_kl.shape)\n",
    "        pi_gp = ( (- grad_kl * delta_x + 1) / self.l**2 -  delta_x**2  / self.l**4 )* exp_kl\n",
    "        print(pi_gp.shape)\n",
    "        return pi_gp*cov\n",
    "\n",
    "\n",
    "    def kernel_ff(self,theta1,theta2):\n",
    "        exp_kl1 = self.exp_kl_eval(theta1,self.source_func_x)\n",
    "        grad_kl1 = self.grad_kl_eval(theta1,self.source_func_x)\n",
    "        exp_kl2 = self.exp_kl_eval(theta2,self.source_func_x)\n",
    "        grad_kl2 = self.grad_kl_eval(theta2,self.source_func_x)\n",
    "\n",
    "        delta_x = (self.source_func_x.reshape(-1,1) - self.source_func_x)\n",
    "\n",
    "        pi_gp1 = ( (grad_kl1 * delta_x + 1) / self.kernel_spatial.l**2 -  delta_x**2  / self.kernel_spatial.l**4 )* exp_kl1\n",
    "        pi_gp2 = ( (grad_kl2 * delta_x + 1) / self.kernel_spatial.l**2 -  delta_x**2  / self.kernel_spatial.l**4 )* exp_kl2\n",
    "\n",
    "        return pi_gp1*pi_gp2*self.kernel_spatial.compute_covariance(self.xf)\n",
    "    \n",
    "    def kernel_pde(self, theta1, theta2):\n",
    "        kuu = self.kernel_parameter.compute_covariance(theta1,theta2)*self.kernel_spatial.compute_covariance(self.x_bc)\n",
    "        kuf = self.kernel_parameter.compute_covariance(theta1,theta2)*self.kernel_uf(theta2,self.x_bc)\n",
    "        Kfu = self.kernel_parameter.compute_covariance(theta1,theta2)*self.kernel_uf(theta1, self.x_bc).T\n",
    "        kff = self.kernel_parameter.compute_covariance(theta1,theta2)*self.kernel_ff(theta1, theta2)\n",
    "\n",
    "        # Concatenate horizontally: [A | B], [C | D]\n",
    "        top = jnp.concatenate([kuu, kuf], axis=1)\n",
    "        bottom = jnp.concatenate([Kfu, kff], axis=1)\n",
    "\n",
    "        # Concatenate vertically\n",
    "        kernel_pde = jnp.concatenate([top, bottom], axis=0) +  1e-10 * jnp.eye(self.dim_total , dtype=jnp.float64)\n",
    "\n",
    "        L = jnp.linalg.cholesky(kernel_pde)\n",
    "        idt = jnp.eye(L.shape[0])\n",
    "        # Step 3: Solve the triangular system directly\n",
    "        z = solve_triangular(L, idt, lower=True)\n",
    "        z_t = solve_triangular(L, idt, lower=True, trans=1)\n",
    "        return L, jnp.dot(z_t, z)\n",
    "    \n",
    "    def matrix_marginal(self,theta1, theta2, x1,x2):\n",
    "        Kuu = self.kernel_spatial.compute_covariance(x1, self.xu)\n",
    "        Kuf = self.kernel_uf(theta2, x1)\n",
    "        matrix_marginal = jnp.concatenate([Kuu, Kuf], axis=1)\n",
    "\n",
    "        KuuT = self.kx.cov(x2, self.xu)\n",
    "        KufT = self.K_uf(theta1, x2)\n",
    "        matrix_marginal_transpose = jnp.concatenate([KuuT, KufT], axis=1).T\n",
    "        return matrix_marginal,matrix_marginal_transpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import qmc\n",
    "\n",
    "class LD_cube:\n",
    "    \n",
    "    def __init__(self, range_para, dimen = 1):\n",
    "        \n",
    "        self.dimen = dimen\n",
    "        self.range_para = self.para_range_setter(range_para)\n",
    "        self.points, self.V = self.para_to_points()\n",
    "        \n",
    "    def para_range_setter(self, range_para):\n",
    "        \n",
    "        if not isinstance(range_para[0],list) and self.dimen == 1:\n",
    "            range_para = [range_para]\n",
    "        elif not isinstance(range_para[0],list) and isinstance(self.dimen,int):\n",
    "            range_para = [range_para] * self.dimen        \n",
    "        \n",
    "        return range_para\n",
    "    \n",
    "    def para_to_points(self):\n",
    "                    \n",
    "        samples =  self.generator()\n",
    "                \n",
    "        V = 1\n",
    "        for j in range(self.dimen):\n",
    "            samples[:,j] = (self.range_para[j][1]-self.range_para[j][0])*samples[:,j] + self.range_para[j][0]\n",
    "            V = V * (self.range_para[j][1]-self.range_para[j][0])\n",
    "        \n",
    "        \n",
    "        return samples, V\n",
    "    \n",
    "class Halton_cube(LD_cube):\n",
    "\n",
    "    def __init__(self, range_para, n, dimen = 1):\n",
    "        \n",
    "        self.n = n\n",
    "        super().__init__(range_para, dimen)\n",
    "    \n",
    "    def generator(self):\n",
    "    \n",
    "        sampler = qmc.Halton(d=self.dimen, scramble=False)\n",
    "        \n",
    "        return sampler.random(n=self.n)\n",
    "    \n",
    "class data:\n",
    "    \"\"\"\n",
    "    Atrributes:\n",
    "    k_dm: dimension of unkonwn parameters\n",
    "    d_tr: number of training points\n",
    "    n_ob: number of observation points\n",
    "    \n",
    "    U: d_tr*k_dm matrix, unknown parameters\n",
    "    Y: d_tr*n_ob matrix, observational data\n",
    "    xx: spatial points corresponding to observations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, U, Y, k_dm, xx = None):\n",
    "        \n",
    "        self.k_dm = k_dm\n",
    "        self.d_tr = int(U.size/k_dm)\n",
    "        self.n_ob = int(Y.size/self.d_tr)\n",
    "        \n",
    "        self.U = U\n",
    "        self.Y = Y\n",
    "        \n",
    "        self.yy = Y.reshape(-1,1)\n",
    "        self.xx = xx\n",
    "        \n",
    "class data_pde:\n",
    "    \"\"\"\n",
    "    Atrributes:\n",
    "    k_dm: dimension of unkonwn parameters\n",
    "    d_tr: number of training points\n",
    "    n_ob: number of observation points\n",
    "    \n",
    "    U: d_tr*k_dm matrix, unknown parameters\n",
    "    Y: d_tr*n_ob matrix, observational data\n",
    "    xx: spatial points corresponding to observations\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, Xbc, ybc, Xf, Yf, k_dm):\n",
    "        \n",
    "        self.k_dm = k_dm\n",
    "        self.d_tr = int(Xf.size/k_dm)\n",
    "        self.n_ob = int(Yf.size/self.d_tr)\n",
    "        \n",
    "        self.Xbc = Xbc\n",
    "        self.ybc = ybc\n",
    "        \n",
    "        self.Xf = Xf\n",
    "        self.Yf = Yf        \n",
    "        \n",
    "        self.yf = Yf.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "\n",
    "class kernel_function:\n",
    "    def __init__(self, dimen):\n",
    "        \n",
    "        self.dimen = dimen\n",
    "        self.K = self.cov\n",
    "    \n",
    "    def parameter_formalizer(self, para):\n",
    "        \n",
    "        para = np.atleast_2d(para).reshape(-1,1)\n",
    "        \n",
    "        return para\n",
    "    \n",
    "    def input_formalizer(self, x):\n",
    "        \n",
    "        return np.atleast_2d(x).reshape(-1,self.dimen)\n",
    "    \n",
    "    def weighted_dist_mat(self, l, x1, x2 = None):\n",
    "        \n",
    "        if x2 is None:\n",
    "            x2 = np.copy(x1)\n",
    "            \n",
    "        x1 = np.atleast_2d(x1).reshape(-1,self.dimen)\n",
    "        x2 = np.atleast_2d(x2).reshape(-1,self.dimen)\n",
    " \n",
    "        return distance_matrix(x1/l,x2/l)\n",
    " \n",
    "    def cov(self, x1, x2, n_ob):\n",
    "        \n",
    "        raise NotImplementedError(\n",
    "            'Function cov not implemented. Please initilize a specific kernel function.')\n",
    "        \n",
    "    def cov_3d(self, x1, x2, n_ob):\n",
    "        cov_mat = self.cov(x1,x2)\n",
    "        a,b = np.shape(cov_mat)\n",
    "        \n",
    "        return np.broadcast_to(cov_mat,(n_ob,a,b))\n",
    "    \n",
    "    def cov_kron(self, x1, x2, n_ob, Spatial_cov = None):\n",
    "        \n",
    "        if Spatial_cov is None:\n",
    "            Spatial_cov = np.eye(n_ob)\n",
    "            \n",
    "        cov_mat = self.cov(x1,x2)\n",
    "        \n",
    "        return np.kron(Spatial_cov,cov_mat)\n",
    "    \n",
    "class kernel_squared_exponential(kernel_function):\n",
    "    \n",
    "    def __init__(self, sigma_square, l, dimen = 1):\n",
    "        \n",
    "        self.sigma_square = sigma_square      \n",
    "        self.l = self.parameter_formalizer(l)  \n",
    "        self.dimen = dimen\n",
    "        \n",
    "        super().__init__(dimen)\n",
    "        \n",
    "    def cov(self, x1, x2 = None):\n",
    "        \n",
    "        d = self.weighted_dist_mat(self.l, x1, x2)\n",
    "        \n",
    "        return self.sigma_square * np.exp(-d**2/2)\n",
    "    \n",
    "    def cov_prime(self, x1, x2):\n",
    "        \n",
    "        return (-(x1.T-x2.T)/(self.l)**2) * self.cov(x1,x2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (6, 10) (6,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Load the data using NumPy\n",
    "u_Theta = np.loadtxt(\"2D_trainingset_N10.txt\")\n",
    "\n",
    "\n",
    "# Number of spatial oberservation points\n",
    "dy = 6\n",
    "X = np.linspace(0,1,dy)\n",
    "\n",
    "# Convert to jax.numpy\n",
    "#u_Theta = jnp.array(data_np)\n",
    "\n",
    "N = 10\n",
    "Theta_train = Halton_cube([-1,1], N+1, 2).points[1:,:]\n",
    "\n",
    "print(Theta_train.shape,u_Theta.shape,X.shape)\n",
    "tr_data = data(Theta_train, u_Theta.T, 2, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, nthetas = 6, 10\n",
    "thetas  = samples_param(nthetas,nparam=2)\n",
    "fem_solver = FEMSolver(np.zeros(2),vert=50)\n",
    "obs_points = np.linspace(0.2,0.8,obs).reshape(-1,1)\n",
    "training_data = np.zeros((nthetas,obs ))\n",
    "\n",
    "for i,theta in enumerate(thetas):\n",
    "    fem_solver.theta = theta\n",
    "    fem_solver.solve()\n",
    "    training_data[i,:] = fem_solver.eval_at_points(obs_points).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gp_regression:\n",
    "    def __init__(self, kernel, tr_data, normalizer = 10**(-10), prior_mean = 0):\n",
    "        \n",
    "        self.k_dm = tr_data.k_dm\n",
    "        self.d_tr = tr_data.d_tr\n",
    "        self.n_ob = tr_data.n_ob\n",
    "        \n",
    "        self.mean = prior_mean\n",
    "        self.k = kernel\n",
    "        self.tr_data = tr_data\n",
    "                \n",
    "        self.normalizer = normalizer\n",
    "                \n",
    "        self.K_inv_y, self.K_tr_tr = self.inverse_precomputer()\n",
    "\n",
    "       \n",
    "    def inverse_precomputer(self):\n",
    "        \n",
    "        K_tr_tr = self.k.cov(self.tr_data.U)   \n",
    "        K_tr_tr = np.kron(K_tr_tr, np.eye(self.n_ob)) + self.normalizer * np.eye(self.n_ob*self.d_tr)\n",
    "\n",
    "        K_inv_y = np.linalg.solve(K_tr_tr, self.tr_data.yy - self.mean)\n",
    "        \n",
    "        return K_inv_y, K_tr_tr\n",
    "    \n",
    "    \n",
    "    def predict_mean(self, test_u):\n",
    "        \n",
    "        K_test_tr = self.k.cov(test_u,self.tr_data.U)\n",
    "        K_test_tr = np.kron(K_test_tr, np.eye(self.n_ob))#self.extend(K_test_tr)\n",
    "            \n",
    "        pred_mean = self.mean + K_test_tr @ self.K_inv_y\n",
    "                \n",
    "        return pred_mean\n",
    "    \n",
    "    \n",
    "    def predict_var(self, test_u):\n",
    "        \n",
    "        K_test_tr = np.kron(self.k.cov(test_u, self.tr_data.U),np.eye(self.n_ob))\n",
    "        K_tr_test = np.kron(self.k.cov(self.tr_data.U, test_u),np.eye(self.n_ob)) \n",
    "        \n",
    "        pred_var = np.kron(self.k.cov(test_u), np.eye(self.n_ob))- K_test_tr @ np.linalg.solve(self.K_tr_tr, K_tr_test)\n",
    "        \n",
    "        return pred_var\n",
    "    \n",
    "    def predict_vars(self, test_u):\n",
    "\n",
    "        pred_vars = np.zeros((len(test_u),self.n_ob,self.n_ob))\n",
    "        \n",
    "        i = 0\n",
    "        for u in test_u:\n",
    "            pred_vars[i,:,:] = self.predict_var(u)\n",
    "            i = i + 1\n",
    "        \n",
    "        return pred_vars\n",
    "\n",
    "    def pred_mean_prime(self, test_u, X = None):\n",
    "        \n",
    "        cov_prime = self.k.cov_prime(test_u,self.tr_data.U)\n",
    "        mean_prime = np.kron(cov_prime, np.eye(self.n_ob)) @ self.K_inv_y\n",
    "        \n",
    "        return mean_prime\n",
    "    \n",
    "    def pred_var_prime(self, test_u):\n",
    "        \n",
    "        pred_var_prime = np.zeros((self.k_dm,self.n_ob,self.n_ob))\n",
    "        cov_prime = self.k.cov_prime(test_u,self.tr_data.U)\n",
    "        \n",
    "        K_tr_test = np.kron(self.k.cov(self.tr_data.U, test_u), np.eye(self.n_ob))\n",
    "        var_prime = np.kron(cov_prime, np.eye(self.n_ob)) @ np.linalg.solve(self.K_tr_tr, K_tr_test) \n",
    "        \n",
    "        for i in range(self.k_dm):\n",
    "            pred_var_prime[i,:,:] = var_prime[i::self.k_dm,:]\n",
    "\n",
    "        return -2 * pred_var_prime\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "class Spc_Gpr(Gp_regression):\n",
    "    \n",
    "    def __init__(self, kernel, tr_data, kernel_x, normalizer = 10**(-10), prior_mean = 0):\n",
    "        \n",
    "        self.kx = kernel_x\n",
    "        self.xx = tr_data.xx\n",
    "        \n",
    "        super().__init__(kernel, tr_data, normalizer, prior_mean)\n",
    "        \n",
    "        \n",
    "    def inverse_precomputer(self):\n",
    "        \n",
    "        K_tr_tr = self.k.cov(self.tr_data.U)         \n",
    "        K_tr_tr = np.kron(K_tr_tr, self.kx.cov(self.xx)) + self.normalizer * np.eye(self.n_ob*self.d_tr)\n",
    "        \n",
    "        K_inv_y = np.linalg.solve(K_tr_tr, self.tr_data.yy - self.mean)\n",
    "        \n",
    "        return K_inv_y, K_tr_tr\n",
    "    \n",
    "    \n",
    "    def predict_mean(self, test_u, X = None):\n",
    "        \n",
    "        if X is None: X = self.xx\n",
    "        \n",
    "        K_test_tr = self.k.cov(test_u,self.tr_data.U)\n",
    "        K_test_tr = np.kron(K_test_tr, self.kx.cov(X, self.xx) )#self.extend(K_test_tr)\n",
    "            \n",
    "        pred_mean = self.mean + K_test_tr @ self.K_inv_y\n",
    "                \n",
    "        return pred_mean.reshape(len(X), -1)\n",
    "    \n",
    "    \n",
    "    def predict_var(self, test_u, X = None):\n",
    "        \n",
    "        if X is None: X = self.xx\n",
    "        \n",
    "        K_test_tr = np.kron(self.k.cov(test_u, self.tr_data.U),self.kx.cov(X, self.xx))\n",
    "        K_tr_test = np.kron(self.k.cov(self.tr_data.U, test_u),self.kx.cov(self.xx, X)) \n",
    "        \n",
    "        pred_var = np.kron(self.k.cov(test_u), self.kx.cov(X))- K_test_tr @ np.linalg.solve(self.K_tr_tr, K_tr_test)\n",
    "        \n",
    "        return pred_var\n",
    "\n",
    "    def pred_mean_prime(self, test_u, X = None):\n",
    "        \n",
    "        if X is None: X = self.xx\n",
    "        \n",
    "        cov_prime = self.k.cov_prime(test_u,self.tr_data.U)\n",
    "        mean_prime = np.kron(cov_prime, self.kx.cov(X, self.xx)) @ self.K_inv_y\n",
    "        \n",
    "        \n",
    "        return mean_prime\n",
    "    \n",
    "    def pred_var_prime(self, test_u, X = None):\n",
    "        \n",
    "        if X is None: X = self.xx\n",
    "        \n",
    "        pred_var_prime = np.zeros((self.k_dm,self.n_ob,self.n_ob))\n",
    "        cov_prime = self.k.cov_prime(test_u,self.tr_data.U)\n",
    "        \n",
    "        K_tr_test = np.kron(self.k.cov(self.tr_data.U, test_u),self.kx.cov(self.xx, X))\n",
    "        var_prime = np.kron(cov_prime, self.kx.cov(X, self.xx)) @ np.linalg.solve(self.K_tr_tr, K_tr_test) \n",
    "        \n",
    "        for i in range(self.k_dm):\n",
    "            pred_var_prime[i,:,:] = var_prime[i::self.k_dm,:]\n",
    "\n",
    "        return -2 * pred_var_prime\n",
    "\n",
    "class PDE_Gpr(Gp_regression):\n",
    "    \n",
    "    def __init__(self, kernel, tr_data, kernel_x, pde_data, normalizer = 10**(-10), prior_mean = 0):\n",
    "        \n",
    "        self.kx = kernel_x\n",
    "        self.xx = tr_data.xx\n",
    "        \n",
    "        self.d_f = pde_data.d_tr       \n",
    "        \n",
    "        self.xxu = pde_data.Xbc\n",
    "        self.yu = pde_data.ybc\n",
    "        \n",
    "        self.xxf = pde_data.Xf\n",
    "        self.yf = pde_data.yf\n",
    " \n",
    "        self.yy = tr_data.yy.reshape(tr_data.d_tr,tr_data.n_ob)[:,1:-1].reshape(-1,1)\n",
    "        super().__init__(kernel, tr_data, normalizer, prior_mean)\n",
    "        \n",
    "   #     spc_Gpr = Spc_Gpr(kernel,tr_data,kernel_x)\n",
    "  #      self.pred_mean_prime = spc_Gpr.pred_mean_prime\n",
    " #       self.pred_var_prime = spc_Gpr.pred_var_prime\n",
    "        \n",
    "    def inverse_precomputer(self):\n",
    "        \n",
    "        K_tr_tr = np.zeros(((self.n_ob-2)*self.d_tr,(self.n_ob-2)*self.d_tr))\n",
    "        \n",
    "        for i in range(self.d_tr):\n",
    "            for j in range(self.d_tr):\n",
    "                K_tr_tr[i*(self.n_ob-2):(i+1)*(self.n_ob-2),j*(self.n_ob-2):(j+1)*(self.n_ob-2)] = self.Condp_spc(self.tr_data.U[i],self.tr_data.U[j],self.xx[1:-1],self.xx[1:-1]) * self.k.cov(self.tr_data.U[i],self.tr_data.U[j]) \n",
    "           \n",
    "        \n",
    "        K_tr_tr = K_tr_tr + self.normalizer*np.eye((self.n_ob-2)*self.d_tr)\n",
    "        \n",
    "        Condmean_XU = np.zeros((self.d_tr*(self.n_ob-2), 1))        \n",
    "        for i in range(self.d_tr):\n",
    "            Condmean_XU[i*(self.n_ob-2):(i+1)*(self.n_ob-2),:] = self.Condp_mean(self.tr_data.U[i],self.xx[1:-1]) \n",
    "            \n",
    "        self.Condmean_XU = Condmean_XU\n",
    "        K_inv_y = np.linalg.solve(K_tr_tr, (self.yy - Condmean_XU))\n",
    "        \n",
    "        return K_inv_y, K_tr_tr\n",
    "    \n",
    "    def K_uf(self, u1, X):\n",
    "        \n",
    "\n",
    "\n",
    "        lam = 1/4\n",
    "        theta = np.array([0.38762262, 0.21646897])\n",
    "        A = np.array([1.05705812, 0.84367913])\n",
    "        omega = np.array([2.15374797, 4.57785946])\n",
    "        \n",
    "        u1 = np.atleast_2d(u1).reshape(1,2)        \n",
    "        \n",
    "        def cossin(u, x):\n",
    "      \n",
    "            z = 0\n",
    "            for j in range(2):\n",
    "                z = z + np.sqrt(theta[j])*u[0][j]*A[j]*(omega[j]*np.cos(omega[j]*x) - lam*omega[j]**2*np.sin(omega[j]*x))\n",
    "\n",
    "            return z\n",
    "\n",
    "        def sincos(u, x):\n",
    "\n",
    "            z = 0\n",
    "            for j in range(2):\n",
    "                z = z + np.sqrt(theta[j])*u[0][j]*A[j]*(np.sin(omega[j]*x) + lam*omega[j]*np.cos(omega[j]*x))\n",
    "                \n",
    "            return z  \n",
    "  \n",
    "        sincos_mat = np.zeros((len(X),self.d_f))\n",
    "        cossin_mat = np.zeros((len(X),self.d_f))        \n",
    "\n",
    "        for j in range(self.d_f):\n",
    "            sincos_mat[:,j] = sincos(u1,self.xxf[j])\n",
    "            cossin_mat[:,j] = cossin(u1,self.xxf[j])\n",
    "        \n",
    "        #print(sincos_mat)\n",
    "        #print(cossin_mat)\n",
    "        diff = X.reshape(-1,1)-self.xxf\n",
    "        #d = self.kx.weighted_dist_mat(1, X, self.xxf)  \n",
    "        LK = ((self.kx.l**2 - diff**2)*np.exp(sincos_mat) - self.kx.l**2 * diff * cossin_mat * np.exp(sincos_mat))/self.kx.l**4\n",
    "        mat = LK * self.kx.cov(X,self.xxf)\n",
    "        \n",
    "        return mat\n",
    "    \n",
    "    def K_ff(self, u1, u2):\n",
    "\n",
    "\n",
    "        lam = 1/4\n",
    "        theta = np.array([0.38762262, 0.21646897])\n",
    "        A = np.array([1.05705812, 0.84367913])\n",
    "        omega = np.array([2.15374797, 4.57785946])\n",
    "        \n",
    "        u1 = np.atleast_2d(u1).reshape(1,2)\n",
    "        u2 = np.atleast_2d(u2).reshape(1,2)  \n",
    "        \n",
    "        def cossin(u, x):\n",
    "            z = 0\n",
    "            for j in range(2):\n",
    "                z = z + np.sqrt(theta[j])*u[0][j]*A[j]*(omega[j]*np.cos(omega[j]*x) - lam*omega[j]**2*np.sin(omega[j]*x))\n",
    "\n",
    "            return z\n",
    "\n",
    "        def sincos(u, x):\n",
    "            z = 0            \n",
    "            for j in range(2):\n",
    "                z = z + np.sqrt(theta[j])*u[0][j]*A[j]*(np.sin(omega[j]*x) + lam*omega[j]*np.cos(omega[j]*x))\n",
    "\n",
    "            return z          \n",
    "        \n",
    "        sincos_mat1 = np.zeros((self.d_f,self.d_f))\n",
    "        sincos_mat2 = np.zeros((self.d_f,self.d_f))\n",
    "        cossin_mat1 = np.zeros((self.d_f,self.d_f))\n",
    "        cossin_mat2 = np.zeros((self.d_f,self.d_f))\n",
    "      \n",
    "\n",
    "        for j in range(self.d_f):\n",
    "            sincos_mat1[:,j] = sincos(u1,self.xxf[j])\n",
    "            cossin_mat1[:,j] = cossin(u1,self.xxf[j])\n",
    "            sincos_mat2[:,j] = sincos(u2,self.xxf[j])\n",
    "            cossin_mat2[:,j] = cossin(u2,self.xxf[j])\n",
    "            \n",
    "        diff = (self.xxf.reshape(-1,1)-self.xxf)/self.kx.l**2\n",
    "        #d = self.kx.weighted_dist_mat(self.kx.l**2, self.xxf, self.xxf)  \n",
    "        LLK = (diff**4 + (cossin_mat2 - cossin_mat1.T)*diff**3 - (6/self.kx.l**2 + cossin_mat1.T*cossin_mat2) *diff**2 + (3*(cossin_mat1.T - cossin_mat2)/self.kx.l**2)*diff + cossin_mat1.T*cossin_mat2/self.kx.l**2 + 3/self.kx.l**4) * np.exp(sincos_mat2) * np.exp(sincos_mat1.T) \n",
    "        mat = LLK * self.kx.cov(self.xxf,self.xxf)\n",
    "\n",
    "        return mat    \n",
    "\n",
    "\n",
    "    def pde_K(self, u1, u2):\n",
    "        \n",
    "        Kuu = self.kx.cov(self.xxu) \n",
    "        Kuf = self.K_uf(u2, self.xxu)\n",
    "        Kfu = self.K_uf(u1, self.xxu).T\n",
    "        Kff = self.K_ff(u1,u2)\n",
    "        \n",
    "        return np.vstack(( np.hstack((Kuu,Kuf)) , np.hstack((Kfu,Kff)) ))\n",
    "\n",
    "    def Condp_spc(self, u1, u2, x1, x2):\n",
    "        \n",
    "        Kuu = self.kx.cov(x1, self.xxu)\n",
    "        Kuf = self.K_uf(u2, x1)\n",
    "        qu  = np.hstack((Kuu, Kuf))\n",
    "        \n",
    "        Kuu = self.kx.cov(x2, self.xxu)\n",
    "        Kuf = self.K_uf(u1, x2)\n",
    "        quT  = np.hstack((Kuu, Kuf)).T\n",
    "        \n",
    "        return (self.kx.cov(x1,x2) - qu@np.linalg.solve(self.pde_K(u1,u2),quT)) \n",
    "    \n",
    "    def Condp_mean(self, u1, x1):\n",
    "        \n",
    "        Kuu = self.kx.cov(x1, self.xxu)\n",
    "        Kuf = self.K_uf(u1, x1)\n",
    "        qu  = np.hstack((Kuu, Kuf))\n",
    "\n",
    "        return (qu @ np.linalg.solve(self.pde_K(u1,u1), np.vstack((self.yu,self.yf))))  \n",
    "    \n",
    "    \n",
    "    def predict_mean(self, u, x = None):\n",
    "        \n",
    "        if x is None: x = self.xx\n",
    "        \n",
    "        Condmean_xu = self.Condp_mean(u,x)\n",
    "                \n",
    "        KuU = np.zeros((len(x),(self.n_ob-2)*self.d_tr))\n",
    "        for i in range(self.d_tr):\n",
    "            KuU[:,i*(self.n_ob-2):(i+1)*(self.n_ob-2)] = self.Condp_spc(u,self.tr_data.U[i],x,self.xx[1:-1])* self.k.cov(u,self.tr_data.U[i])  \n",
    "           \n",
    "        \n",
    "        return Condmean_xu + KuU @ self.K_inv_y\n",
    "    \n",
    "    \n",
    "    def predict_var(self, u, x = None):\n",
    "        \n",
    "        if x is None: x = self.xx\n",
    "        \n",
    "        Kuu = self.Condp_spc(u,u,x,x) * self.k.cov(u,u)\n",
    "        \n",
    "        KuU = np.zeros((len(x),(self.n_ob-2)*self.d_tr))\n",
    "        for i in range(self.d_tr):\n",
    "            KuU[:,i*(self.n_ob-2):(i+1)*(self.n_ob-2)] = self.Condp_spc(u,self.tr_data.U[i],x,self.xx[1:-1]) * self.k.cov(u,self.tr_data.U[i])      \n",
    "        \n",
    "        return Kuu - KuU @ np.linalg.solve(self.K_tr_tr, KuU.T)  \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    def pred_mean_prime(self, u, x = None):\n",
    "        \n",
    "        if x is None: x = self.xx\n",
    "            \n",
    "        KuU = np.zeros((self.k_dm*len(x),(self.n_ob-2)*self.d_tr))        \n",
    "        for i in range(self.d_tr):\n",
    "            KuU[:,i*(self.n_ob-2):(i+1)*(self.n_ob-2)] = np.kron(self.k.cov_prime(u.reshape(1,self.k_dm),self.tr_data.U[i].reshape(1,self.k_dm))  ,self.Condp_spc(u,self.tr_data.U[i],x,self.xx[1:-1]))\n",
    "   \n",
    "        mean_prime = KuU @ self.K_inv_y\n",
    "        \n",
    "        \n",
    "        return mean_prime\n",
    "    \n",
    "    def pred_var_prime(self, u, x = None):\n",
    "        \n",
    "        if x is None: x = self.xx\n",
    "        \n",
    "        pred_var_prime = np.zeros((self.k_dm,(self.n_ob),(self.n_ob)))\n",
    "        cov_prime = self.k.cov_prime(u,self.tr_data.U)\n",
    "\n",
    "        KUu = np.zeros(((self.n_ob-2)*self.d_tr,len(x)))\n",
    "        for i in range(self.d_tr):\n",
    "            KUu[i*(self.n_ob-2):(i+1)*(self.n_ob-2),:] = self.Condp_spc(self.tr_data.U[i], u,self.xx[1:-1],x) * self.k.cov(self.tr_data.U[i],u)\n",
    "            \n",
    "        KuU = np.zeros((self.k_dm*len(x),(self.n_ob-2)*self.d_tr))        \n",
    "        for i in range(self.d_tr):\n",
    "            KuU[:,i*(self.n_ob-2):(i+1)*(self.n_ob-2)] = np.kron(self.k.cov_prime(u.reshape(1,2),self.tr_data.U[i].reshape(1,2))  ,self.Condp_spc(u,self.tr_data.U[i],x,self.xx[1:-1]))\n",
    "        \n",
    "\n",
    "        var_prime = KuU @ np.linalg.solve(self.K_tr_tr, KUu) \n",
    "        \n",
    "        for i in range(self.k_dm):\n",
    "            pred_var_prime[i,:,:] = var_prime[i::self.k_dm,:]\n",
    "\n",
    "        return -2 * pred_var_prime    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data = data(thetas, training_data, 2, obs_points.reshape(-1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized hyperparameter: [1.57410073 4.23240861]\n",
      "[[0.42518855]\n",
      " [0.66964511]\n",
      " [0.92114373]\n",
      " [1.18377489]\n",
      " [1.45010828]\n",
      " [1.69917492]]\n",
      "[[8.26246152e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 8.26246152e-07 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 8.26246153e-07 0.00000000e+00\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 8.26246153e-07\n",
      "  0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  8.26246153e-07 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      "  0.00000000e+00 8.26246153e-07]]\n"
     ]
    }
   ],
   "source": [
    "kernel = kernel_squared_exponential(1,1, dimen = 2)\n",
    "Gpr = Gp_regression(kernel,tr_data)\n",
    "\n",
    "def neg_log_marginal_likelihood(hyp):  # ML_GPR\n",
    "\n",
    "    kernel = kernel_squared_exponential(hyp[0], hyp[1], dimen = 2)\n",
    "    Gpr = Gp_regression(kernel,tr_data)\n",
    "    \n",
    "    K = Gpr.K_tr_tr\n",
    "\n",
    "    eig_v = np.linalg.eigvals(K)\n",
    "\n",
    "    L = np.linalg.cholesky(K)\n",
    "    y = tr_data.yy\n",
    "    nlml = sum(np.log(np.diag(L))) + 0.5*np.matmul(y.T,np.linalg.solve(K,y))[0] + 0.5*60*np.log(2*np.pi)\n",
    "\n",
    "    return nlml\n",
    "\n",
    "neg_log_marginal_likelihood(np.array([1.0, 1.0]))\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "hyp = np.array([1,1])\n",
    "bound = [[10**(-10), 100] for i in range(len(hyp))]\n",
    "hyp1 = minimize(neg_log_marginal_likelihood, hyp, method = 'L-BFGS-B', bounds = bound)\n",
    "print(\"Optimized hyperparameter:\",hyp1.x)\n",
    "\n",
    "kernel = kernel_squared_exponential(*hyp1.x, dimen = 2)\n",
    "Gpr = Gp_regression(kernel,tr_data)\n",
    "\n",
    "print(Gpr.predict_mean(np.array([[0.098, 0.430]])))\n",
    "print(Gpr.predict_var(np.array([[0.098, 0.430]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized hyperparameter: [7.56931582 5.61604521 1.         0.20870697]\n",
      "[[0.42518839]\n",
      " [0.6696448 ]\n",
      " [0.92114354]\n",
      " [1.1837749 ]\n",
      " [1.45010867]\n",
      " [1.69917515]]\n",
      "[[8.26245905e-07 4.01896176e-07 4.63483886e-08 1.26655065e-09\n",
      "  8.13775813e-12 3.50279817e-14]\n",
      " [4.01896176e-07 8.26245565e-07 4.01896350e-07 4.63483081e-08\n",
      "  1.26658187e-09 8.13775965e-12]\n",
      " [4.63483886e-08 4.01896350e-07 8.26245481e-07 4.01896383e-07\n",
      "  4.63483080e-08 1.26655065e-09]\n",
      " [1.26655066e-09 4.63483081e-08 4.01896382e-07 8.26245483e-07\n",
      "  4.01896348e-07 4.63483884e-08]\n",
      " [8.13775849e-12 1.26658186e-09 4.63483081e-08 4.01896349e-07\n",
      "  8.26245568e-07 4.01896176e-07]\n",
      " [3.50281077e-14 8.13775947e-12 1.26655066e-09 4.63483886e-08\n",
      "  4.01896178e-07 8.26245906e-07]]\n"
     ]
    }
   ],
   "source": [
    "def neg_log_marginal_likelihood2(hyp):  # ML_GPR\n",
    "\n",
    "    kernel = kernel_squared_exponential(hyp[0], hyp[1], dimen = 2)\n",
    "    kx = kernel_squared_exponential(1, hyp[3], dimen = 1)\n",
    "    spc_Gpr = Spc_Gpr(kernel,tr_data,kx)\n",
    "    \n",
    "    K = spc_Gpr.K_tr_tr\n",
    "    eig_v = np.linalg.eigvals(K)\n",
    "\n",
    "    L = np.linalg.cholesky(K)\n",
    "    y = tr_data.yy\n",
    "    nlml = 0.5*sum(np.log(np.diag(L))) + 0.5*np.matmul(y.T,np.linalg.solve(K,y))[0] + 0.5*60*np.log(2*np.pi)\n",
    "\n",
    "    return nlml\n",
    "\n",
    "hyp = np.array([8,9,1,0.1])\n",
    "bound = [[10**(-10), 100] for i in range(len(hyp))]\n",
    "hyp2 = minimize(neg_log_marginal_likelihood2, hyp, method = 'L-BFGS-B', bounds = bound)\n",
    "print(\"Optimized hyperparameter:\",hyp2.x)\n",
    "\n",
    "kernel = kernel_squared_exponential(hyp1.x[0],hyp1.x[1], dimen = 2)\n",
    "kx = kernel_squared_exponential(1, hyp[-1], dimen = 1)\n",
    "Gpr = Spc_Gpr(kernel,tr_data,kx)\n",
    "\n",
    "print(Gpr.predict_mean(np.array([[0.098, 0.430]])))\n",
    "print(Gpr.predict_var(np.array([[0.098, 0.430]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7464.237732719945\n",
      "[0.85123893 3.73468095] 0.24955226043524195\n",
      "[0.42498576 0.66927244 0.92063745 1.18318627 1.44953751 1.69878167]\n",
      "[[1.13418078e-06 9.00092243e-07 1.29219642e-06 7.91787320e-07\n",
      "  9.79708166e-07 3.17428682e-07]\n",
      " [1.12018215e-06 1.06976342e-06 1.70701853e-06 1.24662449e-06\n",
      "  1.47410006e-06 5.08108527e-07]\n",
      " [9.36357633e-07 1.03739503e-06 1.83274999e-06 1.60349780e-06\n",
      "  1.82858790e-06 7.07305839e-07]\n",
      " [6.78617033e-07 8.16117738e-07 1.57861400e-06 1.67418471e-06\n",
      "  1.87906303e-06 8.78804086e-07]\n",
      " [4.33871908e-07 5.11793338e-07 1.07440841e-06 1.41191036e-06\n",
      "  1.60469503e-06 9.77894550e-07]\n",
      " [2.45111850e-07 2.46522849e-07 5.63864170e-07 9.60645847e-07\n",
      "  1.13829896e-06 9.54000651e-07]]\n"
     ]
    }
   ],
   "source": [
    "gp = GaussianProcess(thetas,training_data,x_spc=obs_points)\n",
    "\n",
    "print(gp.neg_log_likelihood(jnp.array([1.0, 1.0,1.0])))\n",
    "\n",
    "gp.optimize_nll(jnp.array([1.0, 1.0,1.0]))\n",
    "\n",
    "print(gp.opt_params,gp.opt_params_spc)\n",
    "\n",
    "print(gp.predict_mean(jnp.array([[0.098, 0.430]])))\n",
    "\n",
    "print(gp.predict_var(jnp.array([[0.098, 0.430]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation points for the sourcing function: [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n",
      " 0.66666667 0.77777778 0.88888889 1.        ]\n"
     ]
    }
   ],
   "source": [
    "Xf = np.linspace(0.,1,10)\n",
    "print(\"Observation points for the sourcing function:\",Xf)\n",
    "yf = (4*Xf).reshape(-1,1) # training data\n",
    "Xbc = np.array([0,1])\n",
    "ybc = np.array([0,2]).reshape(-1,1)\n",
    "df = data_pde(Xbc, ybc, Xf, yf, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation points for the sourcing function: [0.         0.11111111 0.22222222 0.33333333 0.44444444 0.55555556\n",
      " 0.66666667 0.77777778 0.88888889 1.        ]\n"
     ]
    }
   ],
   "source": [
    "def neg_log_marginal_likelihood3(hyp):  # ML_GPR\n",
    "    print(hyp)\n",
    "    kernel = kernel_squared_exponential(hyp[0], hyp[1], dimen = 2)\n",
    "    kx = kernel_squared_exponential(1, hyp[3], dimen = 1)\n",
    "    pde_Gpr = PDE_Gpr(kernel,tr_data,kx,df,normalizer=10**(-10))\n",
    "    \n",
    "    K = pde_Gpr.K_tr_tr\n",
    "    eig_v = np.linalg.eigvals(K)\n",
    "\n",
    "    L = np.linalg.cholesky(K)\n",
    "    y = pde_Gpr.yy\n",
    "    nlml = 0.5*sum(np.log(np.diag(L))) + 0.5*np.matmul(y.T,np.linalg.solve(K,y))[0] + 0.5*N*np.log(2*np.pi)\n",
    "\n",
    "    return nlml\n",
    "\n",
    "Xf = np.linspace(0.,1,10)\n",
    "print(\"Observation points for the sourcing function:\",Xf)\n",
    "yf = (4*Xf).reshape(-1,1) # training data\n",
    "Xbc = np.array([0,1])\n",
    "ybc = np.array([0,2]).reshape(-1,1)\n",
    "df = data_pde(Xbc, ybc, Xf, yf, 1)\n",
    "\n",
    "hyp = hyp2.x\n",
    "bound = [[10**(-2), 100] for i in range(len(hyp))]\n",
    "# hyp3 = minimize(neg_log_marginal_likelihood3, hyp, method = 'L-BFGS-B', bounds = bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.19692243,  1.19835481,  0.80876289,  0.26770657, -0.10837473,\n",
       "        -0.21351692, -0.11943765,  0.08673703,  0.37427721,  0.74925105],\n",
       "       [-0.52198305,  0.27218589,  1.04447398,  1.38866671,  1.27227964,\n",
       "         0.94673965,  0.64357261,  0.46291042,  0.43482834,  0.59519506]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel = kernel_squared_exponential(1, 1, dimen = 2)\n",
    "kx = kernel_squared_exponential(1, 1, dimen = 1)\n",
    "pde_Gpr = PDE_Gpr(kernel, tr_data, kx, df)\n",
    "pde_Gpr.K_uf(pde_Gpr.tr_data.U[0],pde_Gpr.xxu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.47849285,  0.7775588 ],\n",
       "       [-0.94599095, -0.32889673],\n",
       "       [ 0.84865827,  0.00117257],\n",
       "       [ 0.22554818, -0.91435519],\n",
       "       [ 0.08650125, -0.90508749],\n",
       "       [-0.91447564,  0.6393561 ],\n",
       "       [ 0.99275248, -0.2209955 ],\n",
       "       [-0.71292978,  0.41615114],\n",
       "       [ 0.09094574, -0.50229902],\n",
       "       [ 0.83499725, -0.94714567]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pde_Gpr.tr_data.U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pde_Gpr.xxu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xf = jnp.linspace(0.,1,10)\n",
    "yf = (4*Xf).reshape(-1,1) # training data\n",
    "x_bc = jnp.array([0,1]).reshape(-1,1)\n",
    "y_bc = jnp.array([0,2]).reshape(-1,1)\n",
    "\n",
    "data_training = {\"parameters_data\": thetas,\n",
    "                 \"solutions_data\": training_data,\n",
    "                 \"x_bc\": x_bc,\n",
    "                 \"y_bc\": y_bc,\n",
    "                 \"source_func_x\": xf,\n",
    "                 \"source_func_f_x\":yf\n",
    "                   }\n",
    "elliptic_gp = Elliptic_PIGP(data_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.        ],\n",
       "       [0.11111111],\n",
       "       [0.22222222],\n",
       "       [0.33333333],\n",
       "       [0.44444444],\n",
       "       [0.55555556],\n",
       "       [0.66666667],\n",
       "       [0.77777778],\n",
       "       [0.88888889],\n",
       "       [1.        ]], dtype=float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_training[\"source_func_x\"].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pde_Gpr.xxu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2) (10, 2) (10, 2)\n",
      "(10, 2)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mul got incompatible shapes for broadcasting: (10, 2), (2, 10).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43melliptic_gp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_uf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpde_Gpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtr_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpde_Gpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxxu\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 68\u001b[0m, in \u001b[0;36mElliptic_PIGP.kernel_uf\u001b[0;34m(self, theta, x)\u001b[0m\n\u001b[1;32m     66\u001b[0m pi_gp \u001b[38;5;241m=\u001b[39m ( (\u001b[38;5;241m-\u001b[39m grad_kl \u001b[38;5;241m*\u001b[39m delta_x \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m  delta_x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m  \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m4\u001b[39m )\u001b[38;5;241m*\u001b[39m exp_kl\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(pi_gp\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpi_gp\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcov\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fenicsx-env/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/anaconda3/envs/fenicsx-env/lib/python3.12/site-packages/jax/_src/numpy/ufunc_api.py:177\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/fenicsx-env/lib/python3.12/site-packages/jax/_src/numpy/ufuncs.py:1187\u001b[0m, in \u001b[0;36m_multiply\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Multiply two arrays element-wise.\u001b[39;00m\n\u001b[1;32m   1162\u001b[0m \n\u001b[1;32m   1163\u001b[0m \u001b[38;5;124;03mJAX implementation of :obj:`numpy.multiply`. This is a universal function,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m  Array([ 0, 10, 20, 30], dtype=int32)\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1186\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m-> 1187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_and(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/fenicsx-env/lib/python3.12/site-packages/jax/_src/lax/lax.py:2072\u001b[0m, in \u001b[0;36mbroadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   2070\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   2071\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2072\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2073\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   2075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: mul got incompatible shapes for broadcasting: (10, 2), (2, 10)."
     ]
    }
   ],
   "source": [
    "elliptic_gp.kernel_uf(pde_Gpr.tr_data.U,pde_Gpr.xxu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0, -1, -2],\n",
       "       [ 1,  0, -1]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnp.array([1,2]).reshape(-1,1) - jnp.array([1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fenicsx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
